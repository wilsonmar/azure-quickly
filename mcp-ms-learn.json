{
  "result": {
    "content": [
      {
        "type": "text",
        "text": "[{\"title\":\"About MCP servers in Azure API Management\",\"content\":\"# About MCP servers in Azure API Management\\n## MCP concepts and architecture\\nAI agents are becoming widely adopted because of enhanced LLM capabilities. However, even the most advanced models face limitations because of their isolation from external data. Each new data source potentially requires custom implementations to extract, prepare, and make data accessible for the models.\\nThe [model context protocol](https://www.anthropic.com/news/model-context-protocol) (MCP) helps solve this problem. MCP is an open standard for connecting AI models and agents with external data sources such as local data sources (databases or computer files) or remote services (systems available over the internet, such as remote databases or APIs).\\nMCP follows a client-server architecture where a host application can connect to multiple servers. Whenever your MCP host or client needs a tool, it connects to the MCP server. The MCP server then connects to, for example, a database or an API. MCP hosts and servers connect with each other through the MCP protocol.\\nThe following diagram illustrates the MCP architecture:\\n![Diagram of model context protocol (MCP) architecture.](https://learn.microsoft.com/en-us/azure/api-management/media/mcp-server-overview/mcp-architecture.png)\\nThe architecture consists of the following components:\\n| Component | Description| \\n|  --- | ---  |\\n| **MCP hosts** | LLM applications such as chat apps or AI assistants in your IDEs (like GitHub Copilot in Visual Studio Code) that need to access external capabilities |\\n| **MCP clients** | Protocol clients, inside the host application, that maintain 1:1 connections with servers |\\n| **MCP servers** | Lightweight programs that each expose specific capabilities and provide context, tools, and prompts to clients |\\n| **MCP protocol** | Transport layer in the middle |\\n\\nThe MCP architecture is built on [JSON-RPC 2.0 for messaging](https://modelcontextprotocol.io/docs/concepts/architecture). Communication between clients and servers occurs over defined transport layers, and supports primarily two modes of operation:\\n1. **Remote MCP servers** - Run as independent processes accessible over the internet using HTTP-based transports (like Streamable HTTP), enabling MCP clients to connect to external services and APIs hosted anywhere.\\n2. **Local MCP servers** MCP clients use standard input/output as a local transport method to connect to MCP servers on the same machine.\",\"contentUrl\":\"https://learn.microsoft.com/en-us/azure/api-management/mcp-server-overview#mcp-concepts-and-architecture\"},{\"title\":\"Connect AI agents to Business Central through MCP server\",\"content\":\"# Connect AI agents to Business Central through MCP server\\n## Feature details\\nMCP is an open API standard that enables intelligent clients to discover, describe, and invoke operations on remote services in a self-describing, plug-and-play way.\\nMCP standardizes how applications provide context to language models. This standardization helps applications integrate seamlessly with different data sources and tools. This open standard connects AI assistants and agents to various systems where data resides, such as content repositories, business tools, and development environments. An MCP-compliant agent uses rich contextual information to act efficiently. A non-MCP-compliant agent doesn\\u0027t have the necessary context.\\nWith the MCP server for Business Central, you can easily connect agents to existing knowledge sources and APIs. You can enable agents to interface directly with Business Central. Actions and knowledge synchronize automatically. This synchronization facilitates real-time updates and the evolution of functionality. This model simplifies agent development and minimizes ongoing maintenance efforts.\\nFor more information, go to [The autonomous enterprise: How generative AI is reshaping business applications](https://www.microsoft.com/dynamics-365/blog/business-leader/2025/05/20/the-autonomous-enterprise-how-generative-ai-is-reshaping-business-applications/?msockid=0868d1b7a20260d00630c41da369618e).\\n## Geographic areas\\nVisit the [Explore Feature Geography](https://aka.ms/FeatureGeographicAvailabilityReport) report for Microsoft Azure areas where this feature is planned or available.\\n## Language availability\\nVisit the [Explore Feature Language](https://aka.ms/FeatureLanguageAvailabilityReport) report for information on this feature\\u0027s availability.\",\"contentUrl\":\"https://learn.microsoft.com/en-us/dynamics365/release-plan/2025wave2/smb/dynamics365-business-central/connect-ai-agents-business-central-through-mcp-server#feature-details\"},{\"title\":\"Get started with .NET AI and the Model Context Protocol\",\"content\":\"# Get started with .NET AI and the Model Context Protocol\\nThe Model Context Protocol (MCP) is an open protocol designed to standardize integrations between AI apps and external tools and data sources. By using MCP, developers can enhance the capabilities of AI models, enabling them to produce more accurate, relevant, and context-aware responses.\\nFor example, using MCP, you can connect your LLM to resources such as:\\n1. Document databases or storage services.\\n2. Web APIs that expose business data or logic.\\n3. Tools that manage files or performing local tasks on a user\\u0027s device.\\nMany Microsoft products already support MCP, including:\\n1. [Copilot Studio](https://www.microsoft.com/microsoft-copilot/blog/copilot-studio/introducing-model-context-protocol-mcp-in-copilot-studio-simplified-integration-with-ai-apps-and-agents/)\\n2. [Visual Studio Code GitHub Copilot agent mode](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode)\\n3. [Semantic Kernel](https://devblogs.microsoft.com/semantic-kernel/integrating-model-context-protocol-tools-with-semantic-kernel-a-step-by-step-guide/).\\nYou can use the MCP C# SDK to quickly create your own MCP integrations and switch between different AI models without significant code changes.\\n### MCP client-server architecture\\nMCP uses a client-server architecture that enables an AI-powered app (the host) to connect to multiple MCP servers through MCP clients:\\n1. **MCP Hosts**: AI tools, code editors, or other software that enhance their AI models using contextual resources through MCP. For example, GitHub Copilot in Visual Studio Code can act as an MCP host and use MCP clients and servers to expand its capabilities.\\n2. **MCP Clients**: Clients used by the host application to connect to MCP servers to retrieve contextual data.\\n3. **MCP Servers**: Services that expose capabilities to clients through MCP. For example, an MCP server might provide an abstraction over a REST API or local data source to provide business data to the AI model.\\nThe following diagram illustrates this architecture:\\n![A diagram showing the architecture pattern of MCP, including hosts, clients, and servers.](https://learn.microsoft.com/en-us/dotnet/ai/media/mcp/model-context-protocol-architecture-diagram.png)\\nMCP client and server can exchange a set of standard messages:\",\"contentUrl\":\"https://learn.microsoft.com/en-us/dotnet/ai/get-started-mcp\"},{\"title\":\"Model Context Protocol (MCP) on Databricks\",\"content\":\"# Model Context Protocol (MCP) on Databricks\\nThis page is an overview of the MCP options on Databricks. [MCP](https://modelcontextprotocol.io/introduction) is an open source standard that connects AI agents to tools, resources, prompts, and other contextual information.\\nThe main benefit of MCP is standardization. You can create a tool once and use it with any agent\\u2014whether it\\u0027s one you\\u0027ve built or a third-party agent. Similarly, you can use tools developed by others, either from your team or from outside your organization.\\nMCP is one of three tool approaches available in Databricks Agent Framework. For guidance on when to choose MCP over Unity Catalog function tools or agent code tools, see [Choose your tool approach](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/agent-tool#tool-comparison).\",\"contentUrl\":\"https://learn.microsoft.com/en-us/azure/databricks/generative-ai/mcp/\"},{\"title\":\"MCP support\",\"content\":\"# MCP support\\nModel Context Protocol (MCP) is an open protocol that standardizes how applications provide context to large language models (LLMs). Starting in AI Shell 1.0.0-preview.6, AI Shell can act as an MCP Host and client to MCP servers. The key participants in the MCP architecture are:\\n1. MCP Host - AI Shell coordinates and manages one or multiple MCP clients\\n2. MCP Client - The client component of AI Shell maintains a connection to an MCP server and obtains context from an MCP server for the MCP host to use\\n3. MCP Server - A program, running locally or hosted remotely, that provides context to MCP clients\\nFor more information about MCP, see the [Architecture Overview](https://modelcontextprotocol.io/docs/learn/architecture) in the official Model Context Protocol documentation.\\nMCP tools enable AI agents to access external tools and services to enhance their capabilities and provide more accurate responses. MCPs can integrate with various APIs, databases, and other resources, allowing agents to retrieve real-time information and perform complex tasks.\",\"contentUrl\":\"https://learn.microsoft.com/en-us/powershell/utility-modules/aishell/how-to/mcp-support?view=ps-modules\"},{\"title\":\"Use MCP servers\",\"content\":\"# Use MCP servers\\nModel Context Protocol (MCP) is an open standard that enables AI models to interact with external tools and services through a unified interface. In Visual Studio, MCP support enhances GitHub Copilot agent mode by allowing you to connect any MCP-compatible server to your agentic coding workflow.\\nThis article guides you through setting up MCP servers and using tools with agent mode in Visual Studio.\\n## Prerequisites\\n1. [Visual Studio 2022 version 17.14](https://learn.microsoft.com/en-us/visualstudio/releases/2022/release-history) or later. We highly recommend the latest servicing release of 17.14 because each release adds MCP features.\\n## How MCP and Visual Studio extend the GitHub Copilot agent\\nMCP support in Visual Studio works as follows:\\n1. MCP clients, such as Visual Studio, connect to MCP servers and request actions on behalf of the AI model.\\n2. MCP servers provide one or more tools that expose specific functionalities through a well-defined interface.\\n3. The protocol defines the message format for communication between clients and servers, including tool discovery, invocation, and response handling.\\nFor example, an MCP server for a file system might provide tools for reading, writing, or searching files and directories. [The official GitHub MCP server](https://github.com/github/github-mcp-server) offers tools to list repositories, create pull requests, or manage issues. MCP servers can run locally on your machine or be hosted remotely. Visual Studio supports both configurations.\\nBy standardizing this interaction, MCP eliminates the need for custom integrations between each AI model and each tool. You can then extend your AI assistant\\u0027s capabilities by simply adding new MCP servers to your workspace. [Learn more about the MCP specification](https://modelcontextprotocol.io/specification/draft).\",\"contentUrl\":\"https://learn.microsoft.com/en-us/visualstudio/ide/mcp-servers?view=vs-2022\"},{\"title\":\"Register and discover remote MCP servers in your API inventory\",\"content\":\"# Register and discover remote MCP servers in your API inventory\\nThis article describes how to use Azure API Center to maintain an inventory (or *registry*) of remote model context protocol (MCP) servers and help stakeholders discover them using the API Center portal. MCP servers expose backend APIs or data sources in a standard way to AI agents and models that consume them.\\n## About MCP servers\\nAI agents are becoming widely adopted because of enhanced large language model (LLM) capabilities. However, even the most advanced models face limitations because of their isolation from external data. Each new data source potentially requires custom implementations to extract, prepare, and make data accessible for the models.\\nThe [model context protocol](https://www.anthropic.com/news/model-context-protocol) (MCP) helps solve this problem. MCP is an open standard for connecting AI models and agents with external data sources such as local data sources (databases or computer files) or remote services (systems available over the internet, such as remote databases or APIs).\\n### MCP architecture\\nMCP follows a client-server architecture where a host application can connect to multiple servers. Whenever your MCP host or client needs a tool, it connects to the MCP server. The MCP server then connects to, for example, a database or an API. MCP hosts and servers connect with each other through the MCP protocol.\\nThe MCP architecture is built on [JSON-RPC 2.0 for messaging](https://modelcontextprotocol.io/docs/concepts/architecture). Communication between clients and servers occurs over defined transport layers, and supports primarily two modes of operation:\\n1. **Remote MCP servers** - MCP clients connect to MCP servers over the internet, establishing a connection using HTTP and server-sent events (SSE), and authorizing the MCP client access to resources on the user\\u0027s account using OAuth.\\n2. **Local MCP servers** MCP clients connect to MCP servers on the same machine, using standard input/output as a local transport method.\",\"contentUrl\":\"https://learn.microsoft.com/en-us/azure/api-center/register-discover-mcp-server\"},{\"title\":\"A2A Client (TypeScript)\",\"content\":\"# A2A Client (TypeScript)\\n## What is an A2A Client?\\nAn A2A client is an agent or application that can proactively send tasks to A2A servers and interact with them using the A2A protocol.\\n## Using A2AClient Directly\\nFor direct control over A2A interactions, you can use the \\u0060A2AClient\\u0060 from the SDK:\\n## Using A2AClientPlugin with ChatPrompt\\nA2A is most effective when used with an LLM. The \\u0060A2AClientPlugin\\u0060 can be added to your chat prompt to allow interaction with A2A agents. Once added, the plugin will automatically configure the system prompt and tool calls to determine if the a2a server is needed for a particular task, and if so, it will do the work of orchestrating the call to the A2A server.\\nTo send a message:\\n\\u0060\\u0060\\u0060typescript\\n// Now we can send the message to the prompt and it will decide if\\n// the a2a agent should be used or not and also manages contacting the agent\\nconst result = await prompt.send(message);\\n\\u0060\\u0060\\u0060\\n### Advanced A2AClientPlugin Configuration\\nYou can customize how the client interacts with A2A agents by providing custom builders:\\n## Sequence Diagram\\nHere\\u0027s how the A2A client works with \\u0060ChatPrompt\\u0060 and \\u0060A2AClientPlugin\\u0060:\\n![alt-text for a2a-client-1.png](https://learn.microsoft.com/en-us/microsoftteams/platform/teams-ai-library/assets/diagrams/a2a-client-1.png)\",\"contentUrl\":\"https://learn.microsoft.com/en-us/microsoftteams/platform/teams-ai-library/typescript/in-depth-guides/ai/a2a/a2a-client#what-is-an-a2a-client\"},{\"title\":\"A2A (Agent-to-Agent) Protocol (TypeScript)\",\"content\":\"# A2A (Agent-to-Agent) Protocol (TypeScript)\\nNote\\nThis package wraps the official [A2A SDK](https://github.com/a2aproject/a2a-js) for both server and client.\\n[What is A2A?](https://a2a-protocol.org/latest/)\\nA2A (Agent-to-Agent) is a protocol designed to enable agents to communicate and collaborate programmatically. This package allows you to integrate the A2A protocol into your Teams app, making your agent accessible to other A2A clients and enabling your app to interact with other A2A servers.\\nInstall the package:\\n\\u0060\\u0060\\u0060bash\\nnpm install @microsoft/teams.a2a\\n\\u0060\\u0060\\u0060\\n## What does this package do?\\n1. **A2A Server**: Enables your Teams agent to act as an A2A server, exposing its capabilities to other agents through the \\u0060/a2a\\u0060 endpoint and serving an agent card at \\u0060/a2a/.well-known/agent-card.json\\u0060.\\n2. **A2A Client**: Allows your Teams app to proactively reach out to other A2A servers as a client, either through direct \\u0060AgentManager\\u0060 usage or integrated with \\u0060ChatPrompt\\u0060 for LLM-driven interactions.\\n## High-level Architecture\\n### A2A Server\\n![alt-text for overview-1.png](https://learn.microsoft.com/en-us/microsoftteams/platform/teams-ai-library/assets/diagrams/overview-1.png)\\n### A2A Client\\n![alt-text for overview-2.png](https://learn.microsoft.com/en-us/microsoftteams/platform/teams-ai-library/assets/diagrams/overview-2.png)\\n## Protocol Details\\nFor detailed information about the A2A protocol, including agent card structure, message formats, and protocol specifications, see the official [A2A Protocol Documentation](https://a2a-protocol.org/latest/specification/).\",\"contentUrl\":\"https://learn.microsoft.com/en-us/microsoftteams/platform/teams-ai-library/typescript/in-depth-guides/ai/a2a/overview\"},{\"title\":\"A2A Server (TypeScript)\",\"content\":\"# A2A Server (TypeScript)\\nIn this guide, you\\u0027ll learn to implement an A2A server to expose your Teams TypeScript app capabilities to other agents using the A2A protocol.\\n## What is an A2A Server?\\nAn A2A server is an agent that exposes its capabilities to other agents using the A2A protocol. With this package, you can make your Teams app accessible to A2A clients.\\n## Adding the A2APlugin\\nTo enable A2A server functionality, add the \\u0060A2APlugin\\u0060 to your Teams app and provide an \\u0060agentCard\\u0060:\\n## Agent Card Exposure\\nThe plugin automatically exposes your agent card at the path \\u0060/a2a/.well-known/agent-card.json\\u0060.\\n## Handling A2A Requests\\nHandle incoming A2A requests by adding an event handler for the \\u0060a2a:message\\u0060 event. You may use \\u0060accumulateArtifacts\\u0060 to iteratively accumulate artifacts for the task, or simply \\u0060respond\\u0060 with the final result.\\nNote\\n1. You must have only a single handler that calls \\u0060respond\\u0060.\\n2. You **must** call \\u0060respond\\u0060 as the last step in your handler. This resolves the open request to the caller.\\n## Sequence Diagram\\n![alt-text for a2a-server-1.png](https://learn.microsoft.com/en-us/microsoftteams/platform/teams-ai-library/assets/diagrams/a2a-server-1.png)\",\"contentUrl\":\"https://learn.microsoft.com/en-us/microsoftteams/platform/teams-ai-library/typescript/in-depth-guides/ai/a2a/a2a-server\"}]"
      }
    ]
  },
  "id": 1,
  "jsonrpc": "2.0"
}